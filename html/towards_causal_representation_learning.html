<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-07-26 Sat 11:49 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Towards Causal Representation Learning</title>
<meta name="author" content="Ketan Agrawal" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="syntax.css" />
<link rel="stylesheet" type="text/css" href="styles.css" />
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
<link rel="manifest" href="/site.webmanifest" />
</head>
<body>
<div id="preamble" class="status">
<header>
    <script src="setup-initial-theme.js"></script>
    <nav style="display: flex; justify-content: space-between; align-items: center;">
        <a href="/" style="color: inherit; text-decoration: none;">ketan.me</a>
        <ul style="display: flex; list-style-type: none; padding: 0; margin: 0;">
            <li style="margin-left: 1rem;"><a href="/blog.html">blog</a></li>
            <li style="margin-left: 1rem;"><a href="/thoughts.html">thoughts</a></li>
            <li style="margin-left: 1rem;"><a href="/experiments.html">garden</a></li>
            <li style="margin-left: 1rem;"><input type="checkbox" id="theme-switcher">
                <label id="theme-switcher-label" for="theme-switcher"></label>
            </li>
        </ul>
    </nav>
</header>
</div>
<div id="content" class="content">
<h1 class="title">Towards Causal Representation Learning
<br />
<span class="subtitle">Last tended to on May 16, 2022</span>
</h1>
<p>
Yoshua Bengio <a href="https://www.youtube.com/watch?v=rKZJ0TJWvTk">talk</a>. Also, the associated <a href="https://arxiv.org/abs/2102.11107">paper</a>.<br />
</p>

<p>
causal representation learning: the discovery of high-level causal variables from low-level observations.<br />
</p>

<p>
In practice, i.i.d. is a bad assumption. Things don&rsquo;t stay the same distribution as train. Current DL systems are brittle.<br />
</p>

<p>
But&#x2026;what assumption can we replace it with, then?<br />
</p>

<p>
how does the brain break knowledge apart into &ldquo;pieces&rdquo; that can be reused? =&gt; <a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">compositionality</a> (thinking decomposition into helper functions in programming.) Examples of compositionality include<br />
</p>
<div id="outline-container-org1fe6986" class="outline-2">
<h2 id="org1fe6986">Systematic Generalization</h2>
<div class="outline-text-2" id="text-org1fe6986">
<p>
Current DL methods overfit the training <b>distribution</b>. That is, if they encounter OOD data, they will perform poorly.<br />
</p>
</div>
<div id="outline-container-org03e33f0" class="outline-3">
<h3 id="org03e33f0">Conscious processing helps humans deal with OOD settings</h3>
<div class="outline-text-3" id="text-org03e33f0">
<p>
We are <i>agents</i>, and agents face a dynamic environment &#x2013; particularly because there are other agents! We want our knowledge to generalize across different places, times, input modalities, goals, etc.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org0bce7b5" class="outline-2">
<h2 id="org0bce7b5">System 1 vs. System 2</h2>
<div class="outline-text-2" id="text-org0bce7b5">
<p>
<b>System 1</b>: Intuitive, fast, unconscious, parallel, non-linguistic, habitual<br />
<b>System 2</b>: Slow, logical, sequential, conscious, linguistic, algorithmic, planning, reasoning<br />
</p>

<p>
Current deep learning systems excel at <a href="thinking_fast_and_slow.html#ID-1a22fb9c-9bc4-4943-9e33-9f08f62409f3">System 1</a> &#x2013; they are fast, intuitive, but brittle. How can we incorporate more <a href="thinking_fast_and_slow.html#ID-62eeec64-5a77-45d2-b386-54fed57e72e0">System 2</a> to allow DL to <i>reason</i> about the world?<br />
</p>
</div>
</div>
<div id="outline-container-org3410344" class="outline-2">
<h2 id="org3410344">Implicit vs. verbalizable knowledge</h2>
<div class="outline-text-2" id="text-org3410344">
<p>
Most of our knowledge is implicit, and not verbalizable. Same for neural networks.<br />
</p>

<p>
Verbalizable knowledge can be explicitly reasoned with, planned with.<br />
</p>
</div>
</div>
<div id="outline-container-orgf3355a2" class="outline-2">
<h2 id="orgf3355a2">Independent mechanisms</h2>
<div class="outline-text-2" id="text-orgf3355a2">
<p>
Hypothesis: We can explain the world by the composition of informationally independent pieces/modules/mechanisms. (Note: not statistically independent, but independent s.t. any causal <a href="causal_inference.html#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a> would affect just one such mechanism.)<br />
</p>
</div>
</div>
<div id="outline-container-org7a7a275" class="outline-2">
<h2 id="org7a7a275">Some System 2 inductive priors</h2>
<div class="outline-text-2" id="text-org7a7a275">
<p>
Sparse causal graph of high-level, semantically meaningful variables.<br />
</p>

<div id="org7444120" class="figure">
<p><img src="origin_image.png" alt="Sparse factor graph." /><br />
</p>
</div>

<p>
Semantic variables are <i>causal</i>: agents, intentions, controllable objects, for example.<br />
</p>

<p>
Changes in distribution are due to causal interventions (in the aforementioned high-level semantic space.) Provided we have the right abstractions, it would only take a few words to describe this change.<br />
</p>

<p>
Everything that&rsquo;s happening can be reported in simple language. (Interesting that this is an example of report/access consciousness.) Mapping from semantic variables &lt;=&gt; sentences<br />
</p>

<p>
&ldquo;generic rules&rdquo; of how things work are shared across instances &#x2013; need variables / functions / some form of indirection.<br />
</p>

<p>
Stability/robustness in <a href="meaning.html#ID-3841138e-363a-4bc2-b1c4-f5abbf973a54">meaning</a> (e.g. of laws of physics,) even with changes in distribution, vs. things that <i>do</i> change. E.g.: early visual layers are stable after childhood. Later things like object recognition can be adapted to very quickly.<br />
</p>

<p>
Causal chains to explain things are short. (Interesting: connection to <a href="cognitive_dissonance.html#ID-8fb8913e-bdd8-4ece-8386-2978b765d7bf">dissonance reduction</a>: we like simple explanations of the world around us (possibly because it helps us streamline our cognition.))<br />
</p>
</div>
</div>
<div id="outline-container-org78906b0" class="outline-2">
<h2 id="org78906b0">What should the causal variables be?</h2>
<div class="outline-text-2" id="text-org78906b0">
<p>
Position and momentum of every particle: computationally intractable.<br />
</p>

<p>
Take inspiration from scientists (and humans in general): we invent <i>high-level abstractions</i> that make the causal structure of the world simpler.<br />
</p>
</div>
</div>
<div id="outline-container-ID-5b73a108-e867-4b92-9949-832840d52869" class="outline-2">
<h2 id="ID-5b73a108-e867-4b92-9949-832840d52869">Agency to Guide Representation Learning &amp; Disentangling</h2>
<div class="outline-text-2" id="text-orgaab16f5">
<p>
(E. Bengio et al, 2017; V Thomas e al 2017; Kim et al ICML 2019)<br />
</p>

<p>
Independent mechanisms: there are ways to modify a single object in the graph (e.g., you can move a chair ‚û°Ô∏èü™ë. )<br />
</p>

<p>
Way that we represent actions &lt;=&gt; objects: there&rsquo;s a bijection there.<br />
</p>

<p>
Connected to the psychological notion of <a href="affordances.html#ID-0cebd56a-9669-4ff0-b93e-8e35d05a2d81">affordances</a>: the way we understand objects is by the things we can do with them.<br />
</p>
</div>
</div>
<div id="outline-container-ID-b4821df4-68e3-43b1-a4f1-c212f0b8d922" class="outline-2">
<h2 id="ID-b4821df4-68e3-43b1-a4f1-c212f0b8d922">What causes changes in distribution?</h2>
<div class="outline-text-2" id="text-org0c56cc4">
<p>
hypothesis to replace i.i.d. assumption: changes in distribution = consequence of an <b>intervention</b> on one/few <b>causes</b> /mechanisms. So, not identically distributed, but pretty similar, if you&rsquo;re in the right high-level <a href="representations.html#ID-c7ba956c-67ad-4b8e-9c7f-f18bc1b2b4ff">representation</a> space. (E.g. if you put shaded glasses on, all the pixels change in basic RGB space &#x2013; but in some high-level semantic space, only one bit changed!)<br />
</p>
</div>
</div>
<div id="outline-container-orgbeffc3e" class="outline-2">
<h2 id="orgbeffc3e">Causal induction from interventional data</h2>
<div class="outline-text-2" id="text-orgbeffc3e">
<p>
How to handle unknown <a href="causal_inference.html#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a>? <i>infer</i> it.<br />
</p>
</div>
</div>
<div id="outline-container-orgfffbf55" class="outline-2">
<h2 id="orgfffbf55">Thoughts, Consciousness, Language</h2>
<div class="outline-text-2" id="text-orgfffbf55">
<p>
If we want better NLP/NLU, we need to ground language in higher-level concepts.<br />
</p>

<p>
<i>Grounded language learning</i>: BabyAI (2019)<br />
</p>
</div>
</div>
<div id="outline-container-orga721bae" class="outline-2">
<h2 id="orga721bae">Core ingredient for conscious processing: <a href="attention.html#ID-2e1955ad-af09-4bcd-8b8d-4a0838e96365">attention</a></h2>
<div class="outline-text-2" id="text-orga721bae">
<p>
Attention enables us to make <i>dynamic</i> connections to the various different &ldquo;modules&rdquo; in the brain. Creates competition between the modules for which deserves attention.<br />
</p>
</div>
</div>
<div id="outline-container-ID-899d0e14-02e5-4858-8f71-8e61e9f59ffa" class="outline-2">
<h2 id="ID-899d0e14-02e5-4858-8f71-8e61e9f59ffa">Going from attention to <a href="consciousness.html#ID-4fba6fb0-e9cc-48b1-875c-a70e1a2dbc9b">consciousness</a></h2>
<div class="outline-text-2" id="text-org71bee77">
<p>
Dehaene et al. &#x2013; workspace theory of consciousness <a href="#citeproc_bib_item_1">[1]</a><br />
</p>
</div>
</div>
<div id="outline-container-org93413df" class="outline-2 no-display">
<h2 id="org93413df">Link</h2>
<div class="outline-text-2" id="text-org93413df">
<p>
<sup><a href="#citeproc_bib_item_2">[2]</a></sup><br />
</p>
</div>
</div>
<div id="outline-container-orga7841f0" class="outline-2">
<h2 id="orga7841f0">Bibliography</h2>
<div class="outline-text-2" id="text-orga7841f0">
<style>.csl-left-margin{float: left; padding-right: 0em;}
 .csl-right-inline{margin: 0 0 0 1em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>
    <div class="csl-left-margin">[1]</div><div class="csl-right-inline">S. Dehaene, J.-P. Changeux, L. Naccache, J. Sackur, and C. Sergent, ‚ÄúConscious, preconscious, and subliminal processing: A testable taxonomy,‚Äù <i>Trends in cognitive sciences</i>, vol. 10, no. 5, pp. 204‚Äì211, May 2006, doi: <a href="https://doi.org/10.1016/j.tics.2006.03.007">10.1016/j.tics.2006.03.007</a>.</div>
  </div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>
    <div class="csl-left-margin">[2]</div><div class="csl-right-inline">B. Sch√∂lkopf <i>et al.</i>, ‚ÄúTowards Causal Representation Learning,‚Äù Feb. 2021, doi: <a href="https://doi.org/10.48550/arXiv.2102.11107">10.48550/arXiv.2102.11107</a>.</div>
  </div>
</div>
</div>
</div>
<div id="outline-container-org11154f5" class="outline-2 references">
<h2 id="org11154f5">Links to &ldquo;Towards Causal Representation Learning&rdquo;</h2>
<div class="outline-text-2" id="text-org11154f5">
</div>
<div id="outline-container-org8fe3dd0" class="outline-3">
<h3 id="org8fe3dd0"><a href="paper_notes.html#ID-d4693400-d612-4531-96cb-da0b8d37b4b0">üìÑ paper notes</a></h3>
<div class="outline-text-3" id="text-org8fe3dd0">
<p>
<a href="towards_causal_representation_learning.html#ID-12dfdb1e-d4ed-476b-be04-98cae7a3deaf">Towards Causal Representation Learning</a><br />
<a href="counterfactual_generative_networks.html#ID-22706d1f-6b5c-4c77-acc2-d8c222b395d5">Counterfactual Generative Networks</a><br />
<a href="language_learning_as_language_use_a_cross_linguistic_model_of_child_language_development_psycnet.html#ID-8ba4e0bd-82cd-4218-b7cd-ed941e74613e">Language learning as language use: A cross-linguistic model of child language development.</a><br />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<!-- copyright Ketan agrawal - on line below-->
<p>&copy; Ketan Agrawal, 2024. <a href="https://x.com/_ketan0">@_ketan0</a>.</p>
<script src="popper.min.js"></script>
<script src="tippy-bundle.umd.min.js"></script>
<script src="tooltips.js"></script>
<script src="setup-theme-switcher.js"></script>
<script src="insert-intext-citation.js"></script>
</div>
</body>
</html>
