<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-07-26 Sat 11:47 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>machine learning</title>
<meta name="author" content="Ketan Agrawal" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="syntax.css" />
<link rel="stylesheet" type="text/css" href="styles.css" />
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
<link rel="manifest" href="/site.webmanifest" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="preamble" class="status">
<header>
    <script src="setup-initial-theme.js"></script>
    <nav style="display: flex; justify-content: space-between; align-items: center;">
        <a href="/" style="color: inherit; text-decoration: none;">ketan.me</a>
        <ul style="display: flex; list-style-type: none; padding: 0; margin: 0;">
            <li style="margin-left: 1rem;"><a href="/blog.html">blog</a></li>
            <li style="margin-left: 1rem;"><a href="/thoughts.html">thoughts</a></li>
            <li style="margin-left: 1rem;"><a href="/experiments.html">garden</a></li>
            <li style="margin-left: 1rem;"><input type="checkbox" id="theme-switcher">
                <label id="theme-switcher-label" for="theme-switcher"></label>
            </li>
        </ul>
    </nav>
</header>
</div>
<div id="content" class="content">
<h1 class="title">machine learning
<br />
<span class="subtitle">Last tended to on May 15, 2022</span>
</h1>
<p>
Supervised learning is an example of <a href="causal_inference.html#ID-d4b17339-7852-4eb6-a399-24e47b354a6c">observational</a> inference &#x2013; we&rsquo;re just looking for associations between variables \(X\) and \(Y\). Aka, we&rsquo;re just learning \(P(Y|X)\).<br />
</p>

<p>
I feel like this thread captures a really interesting divide / contrast of philosophies in machine learning research:<br />
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Researchers in speech recognition, computer vision, and natural language processing in the 2000s were obsessed with accurate representations of uncertainty. <br>1/N</p>&mdash; Yann LeCun (@ylecun) <a href="https://twitter.com/ylecun/status/1525560489216028677?ref_src=twsrc%5Etfw">May 14, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
My goal now is to deeply understand the issues at hand in this thread. I found his mention of factor graphs in the shift to reasoning and planning AI was thought-provoking. I feel that causality and factor graphs and Bayesian and all that are very important. I just don&rsquo;t know quite enough to put the pieces together yet.<br />
</p>
<div id="outline-container-org4283c57" class="outline-2 references">
<h2 id="org4283c57">Links to &ldquo;machine learning&rdquo;</h2>
<div class="outline-text-2" id="text-org4283c57">
</div>
<div id="outline-container-orga0d6533" class="outline-3">
<h3 id="orga0d6533"><a href="artificial_intelligence.html#ID-ef80dd22-3ade-4fd6-98cd-1319eaa454f9">artificial intelligence</a></h3>
<div class="outline-text-3" id="text-orga0d6533">
<p>
Just gonna link this to <a href="machine_learning.html#ID-5b02540a-15ac-4123-86f8-e6ca5420ce27">machine learning</a> for now.<br />
</p>
</div>
</div>
<div id="outline-container-orgd30b15b" class="outline-3">
<h3 id="orgd30b15b"><a href="ablation_studies.html#ID-766f764c-818b-41ff-a7ed-02642696a830">ablation studies</a></h3>
<div class="outline-text-3" id="text-orgd30b15b">
<p>
Ablation studies are effectively using <a href="causal_inference.html#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">interventions</a> (removing parts of your system) to reveal the underlying causal structure of your system. Francois Chollet (creator of Keras) writes about this being useful in a <a href="machine_learning.html#ID-5b02540a-15ac-4123-86f8-e6ca5420ce27">machine learning</a> context:<br />
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Ablation studies are crucial for deep learning research -- can&#39;t stress this enough.<br><br>Understanding causality in your system is the most straightforward way to generate reliable knowledge (the goal of any research). And ablation is a very low-effort way to look into causality.</p>&mdash; François Chollet (@fchollet) <a href="https://twitter.com/fchollet/status/1012721582148550662?ref_src=twsrc%5Etfw">June 29, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</div>
<div id="outline-container-orgff12bc1" class="outline-3">
<h3 id="orgff12bc1"><a href="machine_learning_interviews_book.html#ID-84128eb6-2328-4d5e-a668-c4ccdab9b913">Machine Learning Interviews Book</a></h3>
<div class="outline-text-3" id="text-orgff12bc1">
<p>
<a href="https://huyenchip.com/ml-interviews-book/">https://huyenchip.com/ml-interviews-book/</a><br />
</p>

<p>
This is a book that provides a comprehensive overview of <a href="machine_learning.html#ID-5b02540a-15ac-4123-86f8-e6ca5420ce27">machine learning</a> interview process, including the various machine learning roles, interview pipeline, and practice interview questions.<br />
</p>

<p>
Below are my (WIP) answers to some of the <a href="https://huyenchip.com/ml-interviews-book/contents/part-ii.-questions.html">questions</a> in the book. DISCLAIMER: While these answers are accurate to the best of my knowledge, I can&rsquo;t guarantee their correctness.<br />
</p>
</div>
</div>
<div id="outline-container-orgeee4f78" class="outline-3">
<h3 id="orgeee4f78"><a href="machine_learning_interviews_book.html#ID-84128eb6-2328-4d5e-a668-c4ccdab9b913">Machine Learning Interviews Book</a> <span class="backlinks-outline-path">(<i>Math &gt; Algebra and (little) calculus &gt; Vectors</i>)</span></h3>
<div class="outline-text-3" id="text-orgeee4f78">
<ol class="org-ol">
<li>Dot product<br />
<ol class="org-ol">
<li><p>
[E] <b>What&rsquo;s the geometric interpretation of the dot product of two vectors?</b><br />
The dot product \(\mathbf{x} \cdot \mathbf{y}\) is the length of the projection of \(\mathbf{x}\) onto the unit vector \(\mathbf{\hat{y}}\); algebraically this is equal to \(\lvert\lvert \mathbf{x} \rvert\rvert \cos(\theta)\), where \(\theta\) is the angle between the vectors. Intuitively, the more the two vectors point in the same direction, the higher the dot product will be, as can be seen from the graphic below (where \(DP\) shows the value of the dot product between \(a\) and \(b\).) When the vectors are orthogonal, the dot product is zero.<br />
</p>


<div id="orgb30152f" class="figure">
<p><img src="dot_product.gif" alt="dot_product.gif" /><br />
</p>
</div></li>
<li>[E] <b>Given a vector \(u\), find vector \(v\) of unit length such that the dot product of \(u\) and \(v\) is maximum.</b><br />
This would simply be the unit vector in the direction of \(u\); namely, \(v = \frac{u}{\lvert\lvert u \rvert\rvert}\).<br /></li>
</ol></li>
<li>Outer product<br />
<ol class="org-ol">
<li><p>
[E] <b>Given two vectors \(a=[3,2,1]\) and \(b=[−1,0,1]\), calculate the outer product \(a^T b\).</b><br />
\(a^T b = \begin{bmatrix} 3 \\ 2 \\ 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 1 \end{bmatrix} = \begin{bmatrix} -3 & 0 & 3 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}\)<br />
</p>

<p>
We could also calculate this using numpy, like so:<br />
</p>
<div class="org-src-container" data-language="python">
<pre class="src src-python"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-variable-name">a</span> <span class="org-operator">=</span> np.array([<span class="org-highlight-numbers-number">3</span>,<span class="org-highlight-numbers-number">2</span>,<span class="org-highlight-numbers-number">1</span>])
<span class="org-variable-name">b</span> <span class="org-operator">=</span> np.array([<span class="org-operator">-</span><span class="org-highlight-numbers-number">1</span>,<span class="org-highlight-numbers-number">0</span>,<span class="org-highlight-numbers-number">1</span>])
<span class="org-builtin">print</span>(np.outer(a, b))
</pre>
</div>

<pre class="example">
[[-3  0  3]
 [-2  0  2]
 [-1  0  1]]
</pre></li>

<li><p>
[M] Give an example of how the outer product can be useful in ML.<br />
Potentially&#x2026;in some sort of collaborative filtering system, you&rsquo;d want an outer product between \(m\) user vectors and \(n\) item vectors, to produce a \(m \times n\) similarity matrix?<br />
</p>

<p>
Many applications seem to use the Kronecker product (generalization of the outer product to tensors) &#x2013; don&rsquo;t quite understand them yet, but here are a few:<br />
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1503.05671.pdf">Optimizing Neural Networks with Kronecker-factored Approximate Curvature</a><br /></li>
<li><a href="https://openreview.net/forum?id=rcQdycl0zyk">Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters</a><br /></li>
</ul>

<p>
There has to be something simpler that I&rsquo;m missing.<br />
</p></li>
</ol></li>

<li><p>
[E] <b>What does it mean for two vectors to be linearly independent?</b><br />
A set of vectors is linearly <i>independent</i> if there is no way to nontrivially combine them into the zero vector. What this means is that there should e no way for the vectors to &ldquo;cancel each other out,&rdquo; so to speak; there are no &ldquo;redundant&rdquo; vectors.<br />
</p>

<p>
For only two vectors, this occurs iff (1) neither of them is the zero vector (otherwise the other vector could easily be canceled out by a scalar multiple of \(0\)),  and (2) they are not scalar multiples of each other (otherwise they could simply cancel each other out by multiplying one by a scalar such that it points in the opposite direction as the other.)<br />
</p></li>

<li>[M] <b>Given two sets of vectors \(A=a_1,a_2,a_3,...,a_n\)<br />
and \(B=b_1,b_2,b_3,...,b_m\), How do you check that they share the same basis?</b><br />
I think more precisely we&rsquo;d want to determine whether the <i>vector spaces</i> spanned by \(A\) and \(B\) share the same basis. But for now, let&rsquo;s just gloss over that minor distinction, and refer to the vector spaces as \(A\) and \(B\) as well. With that understanding, we can find a basis of \(A\) by building a maximal linearly independent set, and then if that basis also spans \(B\), then they share the same basis (in fact, that means that \(A\) and \(B\) span the same vector space.)<br /></li>

<li><b>Given \(n\) vectors, each of \(d\) dimensions, what is the dimension of their span?</b><br />
Not enough information. For example, let \(n = 2\) and \(d = 2\). Consider the vector space spanned by \(\{ \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \end{bmatrix}\}\). This vector space is fully spanned by the basis \(\{ \begin{bmatrix} 1 \\ 0 \end{bmatrix}\}\), making the dimension of the span \(1\). However, if we consider the vector space spanned by \(\{ \begin{bmatrix} 2 \\ 3 \end{bmatrix}, \begin{bmatrix} 4 \\ 1 \end{bmatrix}\}\), it is fully spanned by the basis \(\{ \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}\), making the dimension of its span \(2\). So, essentially it depends on the specific values of the vectors, and to what degree they&rsquo;re linearly independent from each other (i.e. the size of the basis.)<br /></li>

<li>Norms and metrics<br />
<ol class="org-ol">
<li><p>
[E] <b>What&rsquo;s a norm? What is \(L_0\), \(L_1\), \(L_2\), \(L_{norm}\)?</b><br />
Can be thought of as a vector&rsquo;s distance from the origin. This becomes relevant in <a href="machine_learning.html#ID-5b02540a-15ac-4123-86f8-e6ca5420ce27">machine learning</a>, when we want to objectively measure and minimize a scalar distance between the ground truth labels and model predictions.<br />
</p>

<p>
So first, we get the vector distance between the ground truth and prediction,  which we can call \(x\). Then, we pass is through one of these norms to obtain a scalar distance:<br />
</p>

<p>
\(L_0\) &ldquo;norm&rdquo; is simply the total number of nonzero elements in a vector. If you consider \(0^0 = 0\), then<br />
\[L_0(x) = \left|x_{1}\right|^{0}+\left|x_{2}\right|^{0}+\cdots+\left|x_{n}\right|^{0}\]<br />
\(L_1\) norm: add up the absolute values of the elements.<br />
\[L_1(x) = \left|x_{1}\right|^{1}+\left|x_{2}\right|^{1}+\cdots+\left|x_{n}\right|^{1}\]<br />
\(L_2\) norm: add up the squares of the elements.<br />
\[L_2(x) = \left|x_{1}\right|^{2}+\left|x_{2}\right|^{2}+\cdots+\left|x_{n}\right|^{2}\]<br />
&#x2026;and so on.<br />
\(L_\infty\) norm is the equal to the maximum absolute value of a component of \(x\). That is:<br />
\[L_\infty = \max_i |x_i|\]<br />
</p>

<p>
I can&rsquo;t find anything on \(L_{norm}\), so maybe that was a typo?<br />
</p></li>

<li>[M] <b>How do norm and metric differ? Given a norm, make a metric. Given a metric, can we make a norm?</b><br />
My understanding is that a norm is simply a mathematical operation on a vector, whereas a (loss) metric is measuring the distance between two things. Given a norm, let&rsquo;s take the \(L_2\) norm, we can easily make a metric, by plugging in a distance vector between ground truth and prediction, as shown in the previous question, and summing the squares of the components. A norm takes in a single variable (i.e. a vector,) whereas a metric takes in two (i.e. ground truth vector and predictions vector.)<br /></li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-org24fe4da" class="outline-3">
<h3 id="org24fe4da"><a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">composability</a> <span class="backlinks-outline-path">(<i>A machine learning example of compositionality</i>)</span></h3>
<div class="outline-text-3" id="text-org24fe4da">
<p>
is that a <a href="computer_vision.html#ID-27d08270-d161-4bb1-8b39-50f28b1ab668">computer vision</a> model&rsquo;s early layers will implement an edge detector, curve detector, etc., which are modular pieces that are then combined in various ways by higher-level layers to form dog detectors, car detectors, etc.<br />
</p>


<div id="org15709c4" class="figure">
<p><img src="cnnlayers.jpg" alt="progression from low-level to high-level features in a CNN." /><br />
</p>
</div>

<p>
But ultimately the ways in which machine learning models, at the moment, can &ldquo;intelligently&rdquo; combine different pieces and parts is limited. See the tweet about DALL-E trying to stack blocks, bless its heart:<br />
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The ways in which <a href="https://twitter.com/hashtag/dalle?src=hash&amp;ref_src=twsrc%5Etfw">#dalle</a> is so incredible (and it is) really put a fine point on the ways in which compositionality is so hard <a href="https://t.co/I6DC4g53MK">pic.twitter.com/I6DC4g53MK</a></p>&mdash; David Madras (@david_madras) <a href="https://twitter.com/david_madras/status/1512573390896480267?ref_src=twsrc%5Etfw">April 8, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>
One step towards compositionality is the idea of neuro-symbolic models. Combining logic-based systems with connectionist methods.<br />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<!-- copyright Ketan agrawal - on line below-->
<p>&copy; Ketan Agrawal, 2024. <a href="https://x.com/_ketan0">@_ketan0</a>.</p>
<script src="popper.min.js"></script>
<script src="tippy-bundle.umd.min.js"></script>
<script src="tooltips.js"></script>
<script src="setup-theme-switcher.js"></script>
<script src="insert-intext-citation.js"></script>
</div>
</body>
</html>
