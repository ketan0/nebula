<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-07-26 Sat 11:48 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>CS236: Deep Generative Models</title>
<meta name="author" content="Ketan Agrawal" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="syntax.css" />
<link rel="stylesheet" type="text/css" href="styles.css" />
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
<link rel="manifest" href="/site.webmanifest" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="preamble" class="status">
<header>
    <script src="setup-initial-theme.js"></script>
    <nav style="display: flex; justify-content: space-between; align-items: center;">
        <a href="/" style="color: inherit; text-decoration: none;">ketan.me</a>
        <ul style="display: flex; list-style-type: none; padding: 0; margin: 0;">
            <li style="margin-left: 1rem;"><a href="/blog.html">blog</a></li>
            <li style="margin-left: 1rem;"><a href="/thoughts.html">thoughts</a></li>
            <li style="margin-left: 1rem;"><a href="/experiments.html">garden</a></li>
            <li style="margin-left: 1rem;"><input type="checkbox" id="theme-switcher">
                <label id="theme-switcher-label" for="theme-switcher"></label>
            </li>
        </ul>
    </nav>
</header>
</div>
<div id="content" class="content">
<h1 class="title">CS236: Deep Generative Models
<br />
<span class="subtitle">Last tended to on May 19, 2022</span>
</h1>
<div id="outline-container-ID-99509c50-67ff-4b27-b719-4816f8e2ed89" class="outline-2">
<h2 id="ID-99509c50-67ff-4b27-b719-4816f8e2ed89">Introduction</h2>
<div class="outline-text-2" id="text-org3d5824c">
<p>
Feynman: &ldquo;What I cannot create, I do not understand&rdquo;<br />
<a href="generative_models.html#ID-94e740e0-9dbb-4f60-99e0-cb1f574fc46f">Generative modeling</a>: &ldquo;What I understand, I can create&rdquo;<br />
</p>
</div>
<div id="outline-container-org3f85364" class="outline-3">
<h3 id="org3f85364">How to generative natural images with a computer?</h3>
<div class="outline-text-3" id="text-org3f85364">
<p>
Generation: High level description =&gt; raw sensory outputs<br />
Inference: raw sensory outputs =&gt; high level description<br />
</p>
</div>
</div>
<div id="outline-container-orgd2f626c" class="outline-3">
<h3 id="orgd2f626c">Statistical Generative Models</h3>
<div class="outline-text-3" id="text-orgd2f626c">
<p>
are learned from data. (Priors are necessary, but not as strict as in Graphics)<br />
</p>

<p>
Data = samples<br />
Priors = parametric (e.g. Gaussian prior), loss function, optimization algo, etc.<br />
</p>

<p>
Image \(x\) =&gt; [probability distribution \(p\)] =&gt; \(p(x)\)<br />
</p>

<p>
Sampling from \(p\) produces realistic samples<br />
</p>
</div>
</div>
<div id="outline-container-org7d38cec" class="outline-3">
<h3 id="org7d38cec">Discriminative vs. generative</h3>
<div class="outline-text-3" id="text-org7d38cec">
<p>
Discriminative model: input \(X\) is given. learns \(P(Y|X)\) (e.g., probability of bedroom given image)<br />
</p>

<p>
<a href="generative_models.html#ID-94e740e0-9dbb-4f60-99e0-cb1f574fc46f">Generative model</a>: input \(X\) is not given. learns \(P(Y,X)\)<br />
</p>
</div>
</div>
<div id="outline-container-ID-2710b5a3-1b64-4e34-883c-86f4b84575ec" class="outline-3">
<h3 id="ID-2710b5a3-1b64-4e34-883c-86f4b84575ec">Conditional generative models</h3>
<div class="outline-text-3" id="text-org71685a9">
<p>
They blur the line between generative and discriminative, because they also condition on some input <a href="features.html#ID-a7203065-7321-4a95-adbe-d38f0d5159c8">features</a>.<br />
</p>

<p>
\(P(X|Y=Bedroom)\)<br />
</p>

<p>
Superresolution: p(high-res signal | low-res signal)<br />
Inpainting: p(full image | mask)<br />
Colorization: p(color image | greyscale)<br />
Translation: p(English text | Chinese text)<br />
Text-to-Image: p(image | caption)<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org955ab29" class="outline-2">
<h2 id="org955ab29">Background</h2>
<div class="outline-text-2" id="text-org955ab29">
</div>
<div id="outline-container-orgc17d27e" class="outline-3">
<h3 id="orgc17d27e">What is a generative model?</h3>
<div class="outline-text-3" id="text-orgc17d27e">
<p>
We are given a dataset of examples, e.g. images of cats. \(P_{data}\) is the underlying distribution that generates the samples. We want to get a \(P_\theta\) that is pretty close to \(P_{data}\).  \(\theta\) is learned by a model, e.g. a neural net.<br />
</p>

<p>
<b>Generation</b>: Then, if we sample \(x_{new} \sim p_{data}(x)\), it should look like a cat.<br />
</p>

<p>
<b>Density estimation</b>: \(p(x)\) should be high if \(x\) looks like a dog, and low otherwise.<br />
</p>

<p>
<b>Unsupervised</b>: We should be able to learn what cats have in common, e.g. ears, tail, etc. (features!)<br />
</p>
</div>
</div>
<div id="outline-container-org06c6afb" class="outline-3">
<h3 id="org06c6afb">Structure through independence</h3>
<div class="outline-text-3" id="text-org06c6afb">
<p>
Consider an input with several components \(X_1, ..., X_n\) (these could be pixels in an image.) If \(X_1, ..., X_n\) are independent, then<br />
\(p\left(x_{1}, \ldots, x_{n}\right)=p\left(x_{1}\right) p\left(x_{2}\right) \cdots p\left(x_{n}\right)\)<br />
</p>

<p>
However, this assumption is too strong &#x2013; oftentimes, components are highly correlated (like pixels in an image.)<br />
</p>

<p>
Chain rule &#x2013; fully general, no assumption on the joint. but the conditionals toward the end become large and intractable. Way too many parameters.<br />
\(p\left(S_{1} \cap S_{2} \cap \cdots \cap S_{n}\right)=p\left(S_{1}\right) p\left(S_{2} \mid S_{1}\right) \cdots p\left(S_{n} \mid S_{1} \cap \ldots \cap S_{n-1}\right)\)<br />
</p>

<p>
Need a better simplifying assumption in the middle&#x2026;<br />
</p>
</div>
<div id="outline-container-org7e9d264" class="outline-4">
<h4 id="org7e9d264">Conditional independence assumption</h4>
<div class="outline-text-4" id="text-org7e9d264">
<p>
\(p(x_1)p(x_2|x_1)...p(x_n|x_{n-1})\)<br />
</p>

<p>
Actually this is just a special case of Bayes network, where it&rsquo;s like a line of nodes<br />
</p>

<p>
x<sub>1</sub> =&gt; x<sub>2</sub> =&gt; x<sub>3</sub> =&gt; x<sub>n</sub>-1 =&gt; x<sub>n</sub>.<br />
</p>
</div>
</div>
<div id="outline-container-orgd8bbe7b" class="outline-4">
<h4 id="orgd8bbe7b">Bayes Network / graphical models:</h4>
<div class="outline-text-4" id="text-orgd8bbe7b">
<p>
This is a directed acyclic graph with one node with each random variable, and one conditional probabiliy distribution per node.<br />
each random variable depends on some parents<br />
\(p\left(x_{1}, \ldots, x_{n}\right)=\prod_{i} p\left(x_{i} \mid x_{Pa_{i}}\right)\)<br />
</p>

<p>
This implies conditional independences between variables that aren&rsquo;t direct parent-child, given their parents(?).<br />
</p>

<p>
Use neural networks to represent the conditional distributions.<br />
</p>
</div>
</div>
<div id="outline-container-org2634e1d" class="outline-4">
<h4 id="org2634e1d">Naive Bayes:</h4>
<div class="outline-text-4" id="text-org2634e1d">
<p>
Assume that all the inputs are independent conditioned on y. (another special case of a Bayes net)<br />
</p>

<p>
directly estimate the conditionals p(xi|y) from data. =&gt; use those + bayes rule to calc p(y|x)<br />
\(p\left(y, x_{1}, \ldots x_{n}\right)=p(y) \prod_{i=1}^{n} p\left(x_{i} \mid y\right)\)<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org8dc85bd" class="outline-3">
<h3 id="org8dc85bd">Discriminative vs. generative</h3>
<div class="outline-text-3" id="text-org8dc85bd">
<p>
p(y,x) = p(x|y)p(y) = p(y|x)p(x)<br />
</p>

<p>
Generative: need to learn/specify both p(y), p(x|y)<br />
Discriminative: just need to learn p(y|x) (X is always given)<br />
</p>

<p>
Discriminative assumes that p(y|x;a) = f(x;a) (assumes that the probability distribution takes a certain functional form.)<br />
</p>

<p>
E.g. logistic regression. Modeling p(y|x) as a linear combination of the inputs =&gt; squeeze with softmax. Decision boundaries are straight lines (assumption of logistic regression.) Logistic does not assume conditional independence like Naive Bayes does.<br />
</p>

<p>
Using a conditional model is only possible when X is always observed. when some Xi are unobserved, the generative model allows us to compute p(Y|X<sub>evidence</sub>) by marginalizing over unseen.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-ID-cb15f12b-6764-4118-a8bb-239333aee169" class="outline-2">
<h2 id="ID-cb15f12b-6764-4118-a8bb-239333aee169">Autoregressive Models</h2>
<div class="outline-text-2" id="text-orge71a268">
<p>
Bayes net with modeling assumptions:<br />
</p>
<ul class="org-ul">
<li>model using chain rule (fully general)<br />
\(p(x) = p(x_1)p(x_2|x_1)p(x_3|x_1, x_2)...p(x_n|x_1, ..., x_{n-1})\)<br /></li>
<li><p>
assume the conditionals take functional form (e.g., a logistic regression)<br />
</p>

<p>
Autoregressive Models are often slower than transformers CNNs etc.<br />
</p></li>
</ul>
</div>
<div id="outline-container-org9cdea9f" class="outline-3">
<h3 id="org9cdea9f">Fully Visible Sigmoid Belief Network (FVSBN)</h3>
<div class="outline-text-3" id="text-org9cdea9f">
<p>
\(p\left(X_{i}=1 \mid x_{<i} ; \alpha^{i}\right)=\sigma\left(\alpha_{0}^{i}+\sum_{j=1}^{i-1} \alpha_{j}^{i} x_{j}\right)\)<br />
</p>
</div>
</div>
<div id="outline-container-org0cdd30d" class="outline-3">
<h3 id="org0cdd30d">Neural Autoregressive Density Estimation (NADE)</h3>
<div class="outline-text-3" id="text-org0cdd30d">
<p>
simple: model as Bernoulli<br />
more classes: model as Categorical<br />
RNADE: continuous- model as mixture of Gaussians<br />
Like FVSBN, but use a 1-hidden-layer neural net:<br />
\(\mathrm{h}_{i}=\sigma\left(A_{i} \mathrm{x}<i+\mathrm{c}_{i}\right)\)<br />
\(p(x_{i} \mid x_{1}, \cdots, x_{i-1} ; \underbrace{A_{i}, \mathrm{c}_{i}, \boldsymbol{\alpha}_{i}, b_{i}}_{\text {parameters }})=\sigma\left(\boldsymbol{\alpha}_{i} \mathrm{~h}_{i}+b_{i}\right)\)<br />
</p>

<p>
Problem: lots of redundant parameters. Solution: &ldquo;tie&rdquo; the weights:<br />
Tie weights to reduce the number of parameters and speed up computation (see blue dots in the figure):<br />
$$<br />
</p>
\begin{array}{r}
\mathrm{h}_{i}=\sigma\left(W_{\cdot,<i} \mathrm{x}<i+\mathrm{c}\right) \\
\hat{x}_{i}=p\left(x_{i} \mid x_{1}, \cdots, x_{i-1}\right)=\sigma\left(\alpha_{i} \mathrm{~h}_{i}+b_{i}\right)
\end{array}
<p>
$$<br />
</p>
</div>
</div>
<div id="outline-container-orge36f243" class="outline-3">
<h3 id="orge36f243">RNADE</h3>
<div class="outline-text-3" id="text-orge36f243">
<p>
\(p\left(x_{i} \mid x_{1}, \cdots, x_{i-1}\right)=\sum_{j=1}^{K} \frac{1}{K} \mathcal{N}\left(x_{i} ; \mu_{i}^{j}, \sigma_{i}^{j}\right)\)<br />
\(\mathrm{h}_{i}=\sigma\left(W_{\cdot,<i} \mathrm{x}<i+\mathrm{c}\right)\)<br />
\(\hat{\boldsymbol{x}}_{i}=\left(\mu_{i}^{1}, \cdots, \mu_{i}^{K}, \sigma_{i}^{1}, \cdots, \sigma_{i}^{K}\right)=f\left(\mathrm{~h}_{i}\right)\)<br />
</p>
</div>
</div>
<div id="outline-container-ID-50bf2de7-e33c-4485-9dcd-b721d7f601a6" class="outline-3">
<h3 id="ID-50bf2de7-e33c-4485-9dcd-b721d7f601a6">Autoregressive Autoencoder: Masked Autoencoder for Distribution Estimation (MADE)</h3>
<div class="outline-text-3" id="text-org38120c4">
<p>
Use masks to disallow certain paths in an autoencoder to make it autoregressive.<br />
</p>

<p>
Solution: use masks to disallow certain paths (Germain et al., 2015). Suppose ordering is \(x_{2}, x_{3}, x_{1}\).<br />
</p>
<ol class="org-ol">
<li>The unit producing the parameters for \(p\left(x_{2}\right)\) is not allowed to depend on any input. Unit for \(p\left(x_{3} \mid x_{2}\right)\) only on \(x_{2}\). And so on&#x2026;<br /></li>
<li>For each unit in a hidden layer, pick a random integer \(i\) in \([1, n-1]\). That unit is allowed to depend only on the first \(i\) inputs (according to the chosen ordering).<br /></li>
<li>Add mask to preserve this invariant: connect to all units in previous layer with smaller or equal assigned number (strictly \(<\) in final layer)<br /></li>
</ol>
</div>
<div id="outline-container-org6fcda13" class="outline-4 references">
<h4 id="org6fcda13">Links to &ldquo;Autoregressive Autoencoder: Masked Autoencoder for Distribution Estimation (MADE)&rdquo;</h4>
<div class="outline-text-4" id="text-org6fcda13">
</div>
<ul class="org-ul">
<li><a id="org9d10639"></a><a href="#ID-206c8dcd-0059-4704-b1a2-d5f1abdbad07">Masked Autoregressive Flow (MAF)</a> <span class="backlinks-outline-path">(<i>Normalizing Flow Models &gt; Autoregressive Models as Normalizing Flow Models &gt; Masked Autoregressive Flow (MAF)</i>)</span><br />
<div class="outline-text-5" id="text-org9d10639">
<p>
Forward mapping from \(z \mapsto x:\)<br />
</p>
<ul class="org-ul">
<li>Let \(x_{1}=\exp \left(\alpha_{1}\right) z_{1}+\mu_{1}\). Compute \(\mu_{2}\left(x_{1}\right), \alpha_{2}\left(x_{1}\right)\)<br /></li>
<li>Let \(x_{2}=\exp \left(\alpha_{2}\right) z_{2}+\mu_{2}\). Compute \(\mu_{3}\left(x_{1}, x_{2}\right), \alpha_{3}\left(x_{1}, x_{2}\right)\)<br /></li>
</ul>
<p>
Sampling is sequential and slow (like autoregressive): \(O(n)\) time<br />
</p>

<p>
Forward mapping from \(z \mapsto x:\)<br />
</p>
<ul class="org-ul">
<li>Let \(x_{1}=\exp \left(\alpha_{1}\right) z_{1}+\mu_{1}\). Compute \(\mu_{2}\left(x_{1}\right), \alpha_{2}\left(x_{1}\right)\)<br /></li>
<li>Let \(x_{2}=\exp \left(\alpha_{2}\right) z_{2}+\mu_{2}\). Compute \(\mu_{3}\left(x_{1}, x_{2}\right), \alpha_{3}\left(x_{1}, x_{2}\right)\)<br /></li>
</ul>
<p>
Sampling is sequential and slow (like autoregressive): \(O(n)\) time<br />
</p>

<p>
Inverse mapping from \(x \mapsto z\) :<br />
</p>
<ul class="org-ul">
<li>Compute all \(\mu_{i}, \alpha_{i}\) (can be done in parallel using e.g., <a href="#ID-50bf2de7-e33c-4485-9dcd-b721d7f601a6">MADE</a>)<br /></li>
<li>Let \(z_{1}=\left(x_{1}-\mu_{1}\right) / \exp \left(\alpha_{1}\right)\) (scale and shift)<br /></li>
<li>Let \(z_{2}=\left(x_{2}-\mu_{2}\right) / \exp \left(\alpha_{2}\right)\)<br /></li>
<li>Let \(z_{3}=\left(x_{3}-\mu_{3}\right) / \exp \left(\alpha_{3}\right) \ldots\)<br /></li>
</ul>
<p>
Jacobian is lower diagonal, hence efficient determinant computation Likelihood evaluation is easy and parallelizable (like MADE)<br />
Layers with different variable orderings can be stacked<br />
</p>
</div>
</li>
</ul>
</div>
</div>
<div id="outline-container-org379952d" class="outline-3">
<h3 id="org379952d">RNNs</h3>
<div class="outline-text-3" id="text-org379952d">
<p>
Challenge: the history for these autoregressive models keeps getting longer and longer. Ideally we&rsquo;d just have a fixed-size &ldquo;summary&rdquo; of the history.<br />
</p>
</div>
</div>
<div id="outline-container-org53e204d" class="outline-3">
<h3 id="org53e204d">Transformers</h3>
<div class="outline-text-3" id="text-org53e204d">
<p>
masked self-attention preserves autoregressive structure.<br />
</p>
</div>
</div>
<div id="outline-container-org97bca28" class="outline-3">
<h3 id="org97bca28">PixelCNN</h3>
<div class="outline-text-3" id="text-org97bca28">
<p>
Masked convolutions preserve raster scan order.<br />
</p>

<p>
Lol u can use these for adversarial attacks &#x2013;<br />
</p>
</div>
</div>
<div id="outline-container-orge545b02" class="outline-3">
<h3 id="orge545b02">WaveNet</h3>
</div>
</div>
<div id="outline-container-org9ed36ae" class="outline-2">
<h2 id="org9ed36ae">Learning a generative model (Maximum Likelihood)</h2>
<div class="outline-text-2" id="text-org9ed36ae">
<p>
We are given a dataset \(D\) of \(m\) samples from \(P_{data}\).<br />
</p>

<p>
We are given a family of models M, our task is to learn a &ldquo;good&rdquo; model M<sub>hat</sub> in M that defines a distribution p<sub>m</sub><sub>hat</sub><br />
</p>

<p>
Can&rsquo;t capture the <b>exact</b> distribution. All we have are samples &#x2013; that&rsquo;s very sparse coverage over the space of all possible samples. (So&#x2026;we need regularization / priors / inductive biases.)<br />
</p>
</div>
<div id="outline-container-org24b8428" class="outline-3">
<h3 id="org24b8428">KL divergence:</h3>
<div class="outline-text-3" id="text-org24b8428">
<p>
&ldquo;distance&rdquo; between two distributions, \(p\) and \(q\).<br />
</p>

<p>
\(D(p \| q)=\sum_{\mathrm{x}} p(\mathrm{x}) \log \frac{p(\mathrm{x})}{q(\mathrm{x})}\)<br />
However, it&rsquo;s not quite a &ldquo;distance,&rdquo; because it&rsquo;s asymmetric. \(D(p \| q) \neq D(q \| p)\)<br />
</p>

<p>
Intuition: we have a known distribution \(q\), and we&rsquo;re trying to minimize the distance to a target distribution \(p\).<br />
</p>
</div>
<div id="outline-container-orga46113f" class="outline-4">
<h4 id="orga46113f">Detour: compression</h4>
<div class="outline-text-4" id="text-orga46113f">
<p>
Generative models are basically compression schemes. Trying to compress the data as well as we can.<br />
</p>

<p>
To compress, it is useful to know the probability distribution the data is sampled from<br />
For example, let X1, · · · , X100 be samples of an unbiased coin. Roughly 50 heads and 50 tails. Optimal compression scheme is to record heads as 0 and tails as 1. In expectation, use 1 bit per sample, and cannot do better<br />
Suppose the coin is biased, and P[H] ≫ P[T]. Then it’s more efficient to uses fewer bits on average to represent heads and more bits to represent tails, e.g.<br />
Batch multiple samples together<br />
Use a short sequence of bits to encode HHHH (common) and a long sequence for TTTT (rare).<br />
Like Morse code: E = •, A = •−, Q = − − •−<br />
KL-divergence: if your data comes from p, but you use a scheme optimized for q, the divergence DKL(p||q) is the number of extra bits you’ll need on average<br />
</p>
</div>
</div>
<div id="outline-container-org058e52e" class="outline-4">
<h4 id="org058e52e">Minimizing KL divergence is equivalent to maximizing the expected log likelihood. =&gt; Maximum Likelihood Estimation</h4>
</div>
</div>
<div id="outline-container-orgf4043c2" class="outline-3">
<h3 id="orgf4043c2">MLE Learning: Stochastic <a href="gradient_descent.html#ID-2a80ce32-c37d-4047-b69b-e0c355d91670">Gradient Descent</a></h3>
</div>
<div id="outline-container-org9f6f157" class="outline-3">
<h3 id="org9f6f157">Empirical risk minimization can easily overfit the data.</h3>
</div>
<div id="outline-container-ID-c8a6b4f0-1424-43e7-913c-f55593b1792f" class="outline-3">
<h3 id="ID-c8a6b4f0-1424-43e7-913c-f55593b1792f">Bias-variance trade off:</h3>
<div class="outline-text-3" id="text-orga340047">
<p>
Bias limitation: If the hypothesis space of functions is very limited, we might not be able to represent the data distribution.<br />
</p>

<p>
Variance limitation: If the hypothesis space is <i>too</i> expressive, it will overfit to the data.<br />
</p>

<p>
How to prevent overfitting? Prefer &ldquo;simpler&rdquo; models (Occam&rsquo;s razor.) <a href="regularization.html#ID-69cfe558-04ed-41b0-8807-922ead5afde3">Regularization</a> in the objective function. Evaluate on validation set while training.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-orgb5fa829" class="outline-2">
<h2 id="orgb5fa829">Latent Variable Models</h2>
<div class="outline-text-2" id="text-orgb5fa829">
</div>
<div id="outline-container-org7ef2b2e" class="outline-3">
<h3 id="org7ef2b2e">Motivation</h3>
<div class="outline-text-3" id="text-org7ef2b2e">
<p>
There are lots of variability in images due to high-level semantic factors: gender, eye color, pose, etc.<br />
</p>

<p>
<b>Idea</b>: model these factors using latent variables \(\mathbf{z}\).<br />
</p>

<p>
If you choose \(\mathbf{z}\) properly, p(x|z) could be a lot simpler than p(x).<br />
</p>

<p>
We could identify the factors of variation using these generative models &#x2013; e.g. p(eye color = blue | x)<br />
</p>
</div>
</div>
<div id="outline-container-org7d3991f" class="outline-3">
<h3 id="org7d3991f">Deep Latent Variable Models</h3>
<div class="outline-text-3" id="text-org7d3991f">
<p>
\(z \sim \mathcal{N}(0, I)\)<br />
</p>

<p>
\(p(x \mid z)=\mathcal{N}\left(\mu_{\theta}(z), \Sigma_{\theta}(z)\right)\) where \(\mu_{\theta}, \Sigma_{\theta}\) are neural networks<br />
</p>

<p>
We <i>hope</i> that z will capture useful factors of variation in an unsupervised manner. Training a classifier on top of z could be a lot easier.<br />
</p>

<p>
Features computer via \(p(z|x)\)<br />
</p>
</div>
</div>
<div id="outline-container-org56054bd" class="outline-3">
<h3 id="org56054bd">Mixture of Gaussians: a shallow latent variable model</h3>
<div class="outline-text-3" id="text-org56054bd">
<p>
A mixture of \(k\) gaussians:<br />
\(z \sim \text{Categorical}(1, ..., K)\)<br />
\(p(x | z = k) = \mathcal{N}(\mu_k, \Sigma_k)\)<br />
</p>
</div>
</div>
<div id="outline-container-orgdc20414" class="outline-3">
<h3 id="orgdc20414">Variational Autoencoder (VAE)</h3>
<div class="outline-text-3" id="text-orgdc20414">
<p>
A mixture of an infinite number of gaussians (since z is continuous):<br />
\(z \sim \mathcal{N}(0, I)\)<br />
</p>

<p>
\(p(x \mid z)=\mathcal{N}\left(\mu_{\theta}(z), \Sigma_{\theta}(z)\right)\) where \(\mu_{\theta}, \Sigma_{\theta}\) are neural networks<br />
</p>

<p>
Simple example:<br />
\(\mu_{\theta}(z)=\sigma(A z+c)=\left(\sigma\left(a_{1} \mathbf{z}+c_{1}\right), \sigma\left(a_{2} \mathbf{z}+c_{2}\right)\right)=\left(\mu_{1}(z), \mu_{2}(z)\right)\)<br />
\(\Sigma_{\theta}(z)=\operatorname{diag}(\exp (\sigma(B \mathbf{z}+d)))=\left(\begin{array}{cc}\exp \left(\sigma\left(b_{1} z+d_{1}\right)\right) & 0 \\ 0 & \exp \left(\sigma\left(b_{2} \mathbf{z}+d_{2}\right)\right)\end{array}\right)\)<br />
\(\theta = A,B,c,d\)<br />
</p>
</div>
</div>
<div id="outline-container-orge55fb22" class="outline-3">
<h3 id="orge55fb22">Good stuff about latent variable models:  complex models, natural for unsupervised learning</h3>
</div>
<div id="outline-container-org714dd42" class="outline-3">
<h3 id="org714dd42">Hard stuff about latent variable models: learning in unsupervised manner is very difficult</h3>
</div>
</div>
<div id="outline-container-ID-02e70065-3d4a-46c4-912a-b73bbd5214ad" class="outline-2">
<h2 id="ID-02e70065-3d4a-46c4-912a-b73bbd5214ad">Normalizing Flow Models</h2>
<div class="outline-text-2" id="text-org83cd1b3">
<p>
Autoregressive models provide tractable likelihoods but no direct mechanism for learning features.<br />
</p>

<p>
Variational autoencoders can learn feature representations (via latent variables \(z\),) but have intractable marginal likelihoods.<br />
</p>

<p>
Normalizing flow models have both latent variables <i>and</i> tractable likelihoods.<br />
</p>

<p>
We want \(p_{\theta}(x)\) to be easy-to-evaluate, and easy-to-sample. The key idea behind flow models is to map simple distributions =&gt; complex distributions through an <b>invertible transformation</b>.<br />
</p>


<p>
This is similar to a VAE:<br />
</p>
<ul class="org-ul">
<li>Start from a simple prior \(z \sim \mathcal{N}(0, I)\)<br /></li>
<li>Transform the sample via \(p(x|z)\)<br /></li>
<li>Problem: \(p_{\theta}(\mathrm{x})=\int p_{\theta}(\mathrm{x}, \mathrm{z}) d \mathrm{z}\) is expensive to compute.<br /></li>
<li>What if we could easily &ldquo;invert&rdquo; \(p(x|z)\) and compute \(p(z|x)\) by design? =&gt; we want \(x = f_{\theta}(z)\) to be a <b>deterministic</b> and <b>invertible</b> function.<br /></li>
</ul>

<p>
We are going to exploit the <b>Change of Variables formula</b>.<br />
</p>
</div>
<div id="outline-container-org3e0917d" class="outline-3">
<h3 id="org3e0917d">Change of variables</h3>
<div class="outline-text-3" id="text-org3e0917d">
<p>
<b>Change of variables (1D case)</b>: If \(X=f(Z)\) and \(f(\cdot)\) is monotone with inverse \(Z=f^{-1}(X)=h(X)\), then:<br />
\[
p_{X}(x)=p_{Z}(h(x))\left|h^{\prime}(x)\right|
\]<br />
</p>

<p>
This result comes from chain rule on the PDF.<br />
</p>

<p>
This allows us to change the distribution of \(X\) in interesting ways &#x2013; start with a simple \(Z\), transform with change-of-variables, and potentially get something much more complex than the prior.<br />
</p>

<p>
\(p_{X}(x)=p_{Z}(z) \frac{1}{f^{\prime}(z)}\)<br />
</p>

<p>
Intuition: if you&rsquo;re expanding in one direction, you&rsquo;re contracting in the other.<br />
</p>

<p>
All this intuition carries over to random vectors (not just random variables.) See slides for more.<br />
</p>

<p>
<b>Change of variables (General case)</b>: The mapping between \(Z\) and \(X\), given by \(f: \mathbb{R}^{n} \mapsto \mathbb{R}^{n}\), is invertible such that \(X=\mathrm{f}(Z)\) and \(Z=f^{-1}(X)\)<br />
\[ p_{X}(\mathrm{x})=p_{Z}\left(\mathrm{f}^{-1}(\mathrm{x})\right)\left|\operatorname{det}\left(\frac{\partial \mathrm{f}^{-1}(\mathrm{x})}{\partial \mathrm{x}}\right)\right|.\]<br />
</p>

<p>
Equivalently, since \(\operatorname{det}\left(A^{-1}\right)=\operatorname{det}(A)^{-1}\) for any invertible matrix \(A\),<br />
\[p_{X}(\mathrm{x})=p_{Z}(\mathrm{z})\left|\operatorname{det}\left(\frac{\partial \mathrm{f}(\mathrm{z})}{\partial \mathrm{z}}\right)\right|^{-1}.\]<br />
</p>

<p>
Note: \(Z\) has to have the <i>same dimensionality</i> as x (so that the mapping is invertible.)<br />
</p>

<p>
It&rsquo;s kinda like a VAE, but \(p(x|z)\) is deterministic. And, crucially, we can directly evaluate the marginal likelihood \(p(x)\); no integration<br />
</p>

<p>
\[ p_{X}(\mathrm{x}|\theta)=p_{Z}\left(\mathrm{f}_\theta^{-1}(\mathrm{x})\right)\left|\operatorname{det}\left(\frac{\partial \mathrm{f}_\theta^{-1}(\mathrm{x})}{\partial \mathrm{x}}\right)\right|.\]<br />
</p>

<p>
Note: \(x, z\) need to be continuous and have the same dimension.<br />
</p>
</div>
</div>
<div id="outline-container-orgaeab431" class="outline-3">
<h3 id="orgaeab431">Flow of transformations</h3>
<div class="outline-text-3" id="text-orgaeab431">
<p>
A flow of transformations: invertible transformations can be composed together.<br />
\[\mathrm{z}_{m}=\mathrm{f}_{\theta}^{m} \circ \cdots \circ \mathrm{f}_{\theta}^{1}\left(\mathrm{z}_{0}\right)=\mathrm{f}_{\theta}^{m}\left(\mathrm{f}_{\theta}^{m-1}\left(\cdots\left(\mathrm{f}_{\theta}^{1}\left(\mathrm{z}_{0}\right)\right)\right)\right) \triangleq \mathrm{f}_{\theta}\left(\mathrm{z}_{0}\right)\]<br />
</p>

<p>
By change of variables<br />
\[
p_{X}(\mathrm{x} ; \theta)=p_{Z}\left(\mathrm{f}_{\theta}^{-1}(\mathrm{x})\right) \prod_{m=1}^{M}\left|\operatorname{det}\left(\frac{\partial\left(\mathrm{f}_{\theta}^{m}\right)^{-1}\left(\mathrm{z}_{m}\right)}{\partial \mathrm{z}_{m}}\right)\right|
.\]<br />
</p>

<p>
By adding more &ldquo;layers&rdquo; in the transformation (i.e. a deeper neural net,) we get something increasingly complexified from the prior.<br />
</p>
</div>
</div>
<div id="outline-container-org40728f4" class="outline-3">
<h3 id="org40728f4">Desiderata for flow models</h3>
<div class="outline-text-3" id="text-org40728f4">
<p>
The prior \(p(z)\) should be simple+efficient; e.g. isotropic Gaussian.<br />
</p>

<p>
The transformations should have tractable evaluation in both directions.<br />
</p>

<p>
Computing the likelihoods \(p(x)\) and \(p(z)\) require you to evaluate the determinant of an \(n \times n\) Jacobian matrix; this is \(O(n^3)\), and way to expensive to do in a learning loop.<br />
</p>

<p>
<b>Key idea</b>: Choose transformations so that their Jacobians have a &ldquo;special&rdquo; structure; e.g. the determinant of a triangular matrix is the product of the diagonals; this is \(O(n)\).<br />
</p>

<p>
^how do we get that to happen? Some possibilities:<br />
</p>
<ul class="org-ul">
<li>Make \(x_i = f_i(z)\) only depend on \(z_{\leq i}\).<br /></li>
<li>More efficient ways of computing Jacobians that are &ldquo;close&rdquo; to the identity matrix (Planar flows paper.)<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org1777ee1" class="outline-3">
<h3 id="org1777ee1">Nonlinear Independent Components Estimation (NICE)</h3>
<div class="outline-text-3" id="text-org1777ee1">
<p>
Partition the variables \(z\) into two disjoint subsets: \(z_{1:d}\) and \(z_{d+1:n}\)<br />
</p>

<p>
Forward mapping (z=&gt;x):<br />
\(x_{1:d} = z_{1:d}\)<br />
\(x_{d+1:n} = z_{d+1:n} + m_\theta(z_{1:d})\) (Where \(m_\theta\) is a neural net)<br />
</p>

<p>
Reverse mapping (x=&gt;z):<br />
\(\mathrm{z}_{1: d}=\mathrm{x}_{1: d}\) (identity transformation)<br />
\(\mathrm{z}_{d+1: n}=\mathrm{x}_{d+1: n}-m_{\theta}\left(\mathrm{x}_{1: d}\right)\)<br />
</p>

<p>
Jacobian:<br />
\(J=\frac{\partial \mathrm{x}}{\partial \mathrm{z}}=\left(\begin{array}{cc}I_{d} & 0 \\ \frac{\partial \mathrm{x}_{d+1: n}}{\partial \mathrm{z}_{1: d}} & I_{n-d}\end{array}\right)\)<br />
\(\operatorname{det}(J)=1\)<br />
</p>

<p>
Since the determinant is 1, it is a <b>volume preserving transformation</b>. (No expanding/contracting)<br />
</p>

<ul class="org-ul">
<li>Invertible<br /></li>
<li>Easy to compute<br /></li>
<li>Tractable marginal likelihood<br /></li>
</ul>

<p>
Additive coupling layers can be composed together.<br />
</p>

<p>
Final layer of NICE applies a rescaling transformation (so we can change the volume.)<br />
Forward mapping \(z \mapsto x:\)<br />
\[
x_{i}=s_{i} z_{i}
\]<br />
where \(s_{i}>0\) is the scaling factor for the $i$-th dimension.<br />
Inverse mapping \(x \mapsto z\) :<br />
\[
z_{i}=\frac{x_{i}}{s_{i}}
\]<br />
Jacobian of forward mapping:<br />
\[\begin{gathered}
J=\operatorname{diag}(\mathrm{s}) \\
\operatorname{det}(J)=\prod_{i=1}^{n} s_{i}
\end{gathered}\]<br />
</p>
</div>
</div>
<div id="outline-container-org680d306" class="outline-3">
<h3 id="org680d306">Real-NVP: Non-volume preserving extension of NICE.</h3>
<div class="outline-text-3" id="text-org680d306">
<p>
Same as NICE, but rescaling happens at each layer.<br />
</p>

<p>
Forward mapping \(z \mapsto x:\)<br />
</p>
<ul class="org-ul">
<li>\(\mathrm{x}_{1: d}=\mathrm{z}_{1: d}\) (identity transformation)<br /></li>
<li>\(\mathrm{x}_{d+1: n}=\mathrm{z}_{d+1: n} \odot \exp \left(\alpha_{\theta}\left(\mathrm{z}_{1: d}\right)\right)+\mu_{\theta}\left(\mathrm{z}_{1: d}\right)\)<br /></li>
<li>\(\mu_{\theta}(\cdot)\) and \(\alpha_{\theta}(\cdot)\) are both neural networks with parameters \(\theta, d\) input units, and \(n-d\) output units \([\odot\) denotes elementwise product \(]\)<br /></li>
</ul>
<p>
Inverse mapping \(x \mapsto z\) :<br />
</p>
<ul class="org-ul">
<li>\(\mathrm{z}_{1: d}=\mathrm{x}_{1: d}\) (identity transformation)<br /></li>
<li>\(\mathrm{z}_{d+1: n}=\left(\mathrm{x}_{d+1: n}-\mu_{\theta}\left(\mathrm{x}_{1: d}\right)\right) \odot\left(\exp \left(-\alpha_{\theta}\left(\mathrm{x}_{1: d}\right)\right)\right)\)<br /></li>
</ul>
<p>
Jacobian of forward mapping:<br />
\[\begin{gathered}
J=\frac{\partial \mathrm{x}}{\partial \mathrm{z}}=\left(\begin{array}{cc}
I_{d} & 0 \\
\frac{\partial \mathrm{x}_{d+1: n}}{\partial \mathrm{z}_{1: d}} & \operatorname{diag}\left(\exp \left(\alpha_{\theta}\left(\mathrm{z}_{1: d}\right)\right)\right)
\end{array}\right) \\
\operatorname{det}(J)=\prod_{i=d+1}^{n} \exp \left(\alpha_{\theta}\left(\mathrm{z}_{1: d}\right)_{i}\right)=\exp \left(\sum_{i=d+1}^{n} \alpha_{\theta}\left(\mathrm{z}_{1: d}\right)_{i}\right)
\end{gathered}\]<br />
Non-volume preserving transformation in general since determinant can be less than or greater than 1<br />
</p>
</div>
</div>
<div id="outline-container-org9af30f2" class="outline-3">
<h3 id="org9af30f2">Autoregressive Models as Normalizing Flow Models</h3>
<div class="outline-text-3" id="text-org9af30f2">
<p>
We can view autoregressive models as flow models.<br />
</p>

<p>
Consider a Gaussian autoregressive model:<br />
\[
p(\mathrm{x})=\prod_{i=1}^{n} p\left(x_{i} \mid \mathrm{x}<i\right)
\]<br />
such that \(p\left(x_{i} \mid \mathrm{x}_{<i}\right)=\mathcal{N}\left(\mu_{i}\left(x_{1}, \cdots, x_{i-1}\right), \exp \left(\alpha_{i}\left(x_{1}, \cdots, x_{i-1}\right)\right)^{2}\right)\) Here, \(\mu_{i}(\cdot)\) and \(\alpha_{i}(\cdot)\) are neural networks for \(i>1\) and constants for \(i=1\)<br />
Sampler for this model:<br />
</p>
<ul class="org-ul">
<li>Sample \(z_{i} \sim \mathcal{N}(0,1)\) for \(i=1, \cdots, n\)<br /></li>
<li>Let \(x_{1}=\exp \left(\alpha_{1}\right) z_{1}+\mu_{1}\). Compute \(\mu_{2}\left(x_{1}\right), \alpha_{2}\left(x_{1}\right)\)<br /></li>
<li>Let \(x_{2}=\exp \left(\alpha_{2}\right) z_{2}+\mu_{2}\). Compute \(\mu_{3}\left(x_{1}, x_{2}\right), \alpha_{3}\left(x_{1}, x_{2}\right)\)<br /></li>
<li>Let \(x_{3}=\exp \left(\alpha_{3}\right) z_{3}+\mu_{3} \ldots\)<br /></li>
</ul>
<p>
Flow interpretation: transforms samples from the standard Gaussian \(\left(z_{1}, z_{2}, \ldots, z_{n}\right)\) to those generated from the model \(\left(x_{1}, x_{2}, \ldots, x_{n}\right)\) via invertible transformations (parameterized by \(\left.\mu_{i}(\cdot), \alpha_{i}(\cdot)\right)\)<br />
</p>
</div>
<div id="outline-container-ID-206c8dcd-0059-4704-b1a2-d5f1abdbad07" class="outline-4">
<h4 id="ID-206c8dcd-0059-4704-b1a2-d5f1abdbad07">Masked Autoregressive Flow (MAF)</h4>
<div class="outline-text-4" id="text-org5f04117">
<p>
Forward mapping from \(z \mapsto x:\)<br />
</p>
<ul class="org-ul">
<li>Let \(x_{1}=\exp \left(\alpha_{1}\right) z_{1}+\mu_{1}\). Compute \(\mu_{2}\left(x_{1}\right), \alpha_{2}\left(x_{1}\right)\)<br /></li>
<li>Let \(x_{2}=\exp \left(\alpha_{2}\right) z_{2}+\mu_{2}\). Compute \(\mu_{3}\left(x_{1}, x_{2}\right), \alpha_{3}\left(x_{1}, x_{2}\right)\)<br /></li>
</ul>
<p>
Sampling is sequential and slow (like autoregressive): \(O(n)\) time<br />
</p>

<p>
Forward mapping from \(z \mapsto x:\)<br />
</p>
<ul class="org-ul">
<li>Let \(x_{1}=\exp \left(\alpha_{1}\right) z_{1}+\mu_{1}\). Compute \(\mu_{2}\left(x_{1}\right), \alpha_{2}\left(x_{1}\right)\)<br /></li>
<li>Let \(x_{2}=\exp \left(\alpha_{2}\right) z_{2}+\mu_{2}\). Compute \(\mu_{3}\left(x_{1}, x_{2}\right), \alpha_{3}\left(x_{1}, x_{2}\right)\)<br /></li>
</ul>
<p>
Sampling is sequential and slow (like autoregressive): \(O(n)\) time<br />
</p>

<p>
Inverse mapping from \(x \mapsto z\) :<br />
</p>
<ul class="org-ul">
<li>Compute all \(\mu_{i}, \alpha_{i}\) (can be done in parallel using e.g., <a href="#ID-50bf2de7-e33c-4485-9dcd-b721d7f601a6">MADE</a>)<br /></li>
<li>Let \(z_{1}=\left(x_{1}-\mu_{1}\right) / \exp \left(\alpha_{1}\right)\) (scale and shift)<br /></li>
<li>Let \(z_{2}=\left(x_{2}-\mu_{2}\right) / \exp \left(\alpha_{2}\right)\)<br /></li>
<li>Let \(z_{3}=\left(x_{3}-\mu_{3}\right) / \exp \left(\alpha_{3}\right) \ldots\)<br /></li>
</ul>
<p>
Jacobian is lower diagonal, hence efficient determinant computation Likelihood evaluation is easy and parallelizable (like MADE)<br />
Layers with different variable orderings can be stacked<br />
</p>
</div>
<ul class="org-ul">
<li><a id="orgda08894"></a>Links to &ldquo;Masked Autoregressive Flow (MAF)&rdquo;<br />
<div class="outline-text-5" id="text-orgda08894">
</div>
<ul class="org-ul">
<li><a id="org0aec130"></a><a href="#ID-02e70065-3d4a-46c4-912a-b73bbd5214ad">Normalizing Flow Models</a> <span class="backlinks-outline-path">(<i>Normalizing Flow Models &gt; Autoregressive Models as Normalizing Flow Models &gt; Inverse Autoregressive Flow (IAF)</i>)</span><br />
<div class="outline-text-6" id="text-org0aec130">
<p>
Identical to <a href="#ID-206c8dcd-0059-4704-b1a2-d5f1abdbad07">MAF</a>, but change the roles of z and x.<br />
</p>
</div>
</li>
</ul>
</li>
</ul>
</div>
<div id="outline-container-org017f48c" class="outline-4">
<h4 id="org017f48c">Inverse Autoregressive Flow (IAF)</h4>
<div class="outline-text-4" id="text-org017f48c">
<p>
Identical to <a href="#ID-206c8dcd-0059-4704-b1a2-d5f1abdbad07">MAF</a>, but change the roles of z and x.<br />
</p>
</div>
</div>
<div id="outline-container-orga58538e" class="outline-4">
<h4 id="orga58538e">Computational tradeoffs of MAF vs. IAF</h4>
<div class="outline-text-4" id="text-orga58538e">
<p>
MAF: Fast likelihood evaluation, slow sampling<br />
^good for training<br />
</p>

<p>
IAF: Fast sampling, slow likelihood evaluation<br />
^good for inference<br />
</p>
</div>
</div>
<div id="outline-container-org3976475" class="outline-4">
<h4 id="org3976475">Parallel Wavenet</h4>
<div class="outline-text-4" id="text-org3976475">
<p>
Idea: best of both worlds&#x2026;teacher MAF model, student IAF model. First train MAF model normally. Then train IAF model to minimize divergence with MAF model. Use IAF model at test-time.<br />
</p>

<p>
<b>Probability density distillation</b>: Student distribution is trained to minimize the \(\mathrm{KL}\) divergence between student \((s)\) and teacher \((t)\)<br />
\[
D_{\mathrm{KL}}(s, t)=E_{\mathrm{x} \sim s}[\log s(\mathrm{x})-\log t(\mathrm{x})]
\]<br />
</p>

<p>
Evaluating and optimizing Monte Carlo estimates of this objective requires:<br />
</p>
<ul class="org-ul">
<li>Samples \(x\) from student model (IAF)<br /></li>
<li>Density of \(x\) assigned by student model (IAF)<br /></li>
<li>Density of \(x\) assigned by teacher model (MAF)<br /></li>
</ul>

<p>
All operations above can be implemented efficiently.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-orga68e8d7" class="outline-3">
<h3 id="orga68e8d7">Invertible CNNs</h3>
<div class="outline-text-3" id="text-orga68e8d7">
<p>
It&rsquo;s possible to change a convolutional architecture to become invertible.<br />
</p>

<p>
We can use masked convolutions to enforce ordering =&gt; Jacobian is lower triangular + easy to compute. If all the diagonal elements of Jacobian are positive, the transformation is invertible.<br />
</p>

<p>
The point is, you can train a ResNet normally, then invert + use as a flow model.<br />
</p>
</div>
<div id="outline-container-org3636707" class="outline-4">
<h4 id="org3636707">MintNet</h4>
<div class="outline-text-4" id="text-org3636707">
<p>
uses masked/causal convolutions in a way enforces ordering, makes the Jacobian triangular, makes the transformation invertible..<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org7a6d4b8" class="outline-3">
<h3 id="org7a6d4b8">Gaussianization flows</h3>
<div class="outline-text-3" id="text-org7a6d4b8">
<p>
Let \(X=f_{\theta}(Z)\) be a flow model with Gaussian prior \(Z \sim \mathcal{N}(0, I)=p_{Z}\), and let \(\tilde{X} \sim p_{\text {data }}\) be a random vector distributed according to the true data distribution.<br />
</p>

<p>
Flow models are trained with maximum likelihood to minimize the \(\mathrm{KL}\) divergence \(D_{\mathrm{KL}}\left(p_{\text {data }} \| p_{\theta}(x)\right)=D_{\mathrm{KL}}\left(p_{\tilde{X}} \| p_{X}\right).\) Gaussian samples transformed through \(f_{\theta}\) should be distributed as the data.<br />
</p>

<p>
It can be shown that \(D_{\mathrm{KL}}\left(p_{\tilde{X}} \| p_{X}\right)=D_{\mathrm{KL}}\left(p_{f_{\theta}^{-1}(\tilde{X})} \| p_{f_{\theta}^{-1}(X)}\right)=D_{\mathrm{KL}}\left(p_{f_{\theta}^{-1}(\tilde{X})} \| p_{Z}\right).\)Ideally, data samples transformed through \(f_{\theta}^{-1}\) should be distributed as Gaussian (Hence &ldquo;Gaussianizing.&rdquo;) Then, we can easily turn it around and efficiently generate new data samples from Gaussian prior. So, how can we achieve this?<br />
</p>
</div>
<div id="outline-container-orgdfe1780" class="outline-4">
<h4 id="orgdfe1780">Inverse CDF trick</h4>
<div class="outline-text-4" id="text-orgdfe1780">
<p>
Inverse CDF gives you data samples from a distribution. E.g. Inverse Gaussian composed with \(F_{\text{data}}\) can give you gaussian data.<br />
</p>
</div>
</div>
<div id="outline-container-org8e3f40c" class="outline-4">
<h4 id="org8e3f40c">Step 1: Dimension-wise Gaussianization</h4>
</div>
<div id="outline-container-orgcaa1269" class="outline-4">
<h4 id="orgcaa1269">Step 2: apply a rotation matrix to the transformed data</h4>
</div>
<div id="outline-container-orgf20c6ce" class="outline-4">
<h4 id="orgf20c6ce">repeat Step 1 and Step 2 (&ldquo;stack&rdquo; these models) =&gt; eventually Gaussian.</h4>
</div>
</div>
</div>
<div id="outline-container-org004a194" class="outline-2">
<h2 id="org004a194">Generative Adversarial Networks (GANs)</h2>
<div class="outline-text-2" id="text-org004a194">
<p>
Autoregressive and VAEs use maximum likelihood training over the marignal likelihood (or an approximation, at least.) But why maximum likelihood? =&gt; higher likelihood = better lossless compression.<br />
</p>

<p>
But&#x2026;let&rsquo;s say our goal isn&rsquo;t compression, but high-quality samples. Granted&#x2026;the optimal generative model will maximize <i>both</i> sample quality and log-likelihood. However, in real life, nothing is perfect, and for imperfect models, high likelihood != good sample quality. (Can have great likelihoods, but terrible samples, or terrible likelihoods but great samples.)<br />
</p>

<p>
<b>Likelihood-free learning</b> consider objectives that do not depend directly on a likelihood function.<br />
</p>

<p>
When we don&rsquo;t have access to likelihood, we can&rsquo;t depend on KL divergence to optimize. Need a new way of quantifying distance.<br />
</p>

<p>
Given a finite set of samples from two distributions \(S_{1}=\{\mathbf{x} \sim P\}\) and \(S_{2}=\{\mathbf{x} \sim Q\}\), how can we tell if these samples are from the same distribution? (i.e., \(P=Q\) ?) =&gt; two-sample test from statistics.<br />
</p>

<p>
New objective: train the generative model to minimize a two-sample test objective between \(S_1\) and \(S_2\). But&#x2026;that&rsquo;s hard to directly optimize those two to converge.<br />
</p>

<p>
But&#x2026;ok in the generative modeling setting, we know that \(S_1\) and \(S_2\) come from different distributions, the data distribution and the model&rsquo;s approximation of that. let&rsquo;s exploit that we know that label and learn a statistic that <i>maximizes</i> a suitable notion of distance between \(S_1\) and \(S_2\).<br />
</p>
</div>
<div id="outline-container-org9046669" class="outline-3">
<h3 id="org9046669">Two-sample test via a Discriminator</h3>
<div class="outline-text-3" id="text-org9046669">
<p>
A neural net that tries to distinguish &ldquo;real&rdquo; from &ldquo;fake&rdquo; samples.<br />
</p>

<p>
Maximize the two-sample test objective (in support of the hypotehsis \(p_{\text {data }} \neq p_{\theta}\))<br />
Training objective for discriminator:<br />
</p>

<p>
\[
\max _{D} V(G, D)=E_{\mathbf{x} \sim p_{\text {data }}}[\log D(\mathbf{x})]+E_{\mathbf{x} \sim p_{G}}[\log (1-D(\mathbf{x}))]
\]<br />
For a fixed generator \(G\), the discriminator is performing binary classification with the cross entropy objective<br />
</p>
<ul class="org-ul">
<li>Assign probability 1 to true data points \(\mathbf{x} \sim p_{\text {data }}\)<br /></li>
<li>Assing probability 0 to fake samples \(\mathbf{x} \sim p_{G}\)<br /></li>
</ul>
<p>
Optimal discriminator<br />
\[
D_{G}^{*}(\mathbf{x})=\frac{p_{\mathrm{data}}(\mathbf{x})}{p_{\mathrm{data}}(\mathbf{x})+p_{G}(\mathbf{x})}
\]<br />
</p>

<p>
(We don&rsquo;t want to use likelihoods, though.)<br />
</p>
</div>
</div>
<div id="outline-container-orgbcdd18a" class="outline-3">
<h3 id="orgbcdd18a">GANs are basically a two-player minimax game between a <b>generator</b> and <b>discriminator</b>.</h3>
</div>
<div id="outline-container-org926c62d" class="outline-3">
<h3 id="org926c62d">Generator</h3>
<div class="outline-text-3" id="text-org926c62d">
<p>
Directed, latent variable model with a deterministic mapping between \(z\) and \(x\), \(G_\theta\).<br />
Training objective for generator:<br />
\[
\min _{G} \max _{D} V(G, D)=E_{\mathbf{x} \sim p_{\text {data }}}[\log D(\mathbf{x})]+E_{\mathbf{x} \sim p_{G}}[\log (1-D(\mathbf{x}))]
\]<br />
For the optimal discriminator \(D_{G}^{*}(\cdot)\), we have<br />
$$<br />
</p>
\begin{gathered}
V\left(G, D_{G}^{*}(\mathbf{x})\right) \\
=E_{\mathbf{x} \sim p_{\text {data }}}\left[\log \frac{p_{\text {data }}(\mathbf{x})}{p_{\text {data }}(\mathbf{x})+p_{G}(\mathbf{x})}\right]+E_{\mathbf{x} \sim p_{G}}\left[\log \frac{p_{G}(\mathbf{x})}{p_{\text {data }}(\mathbf{x})+p_{G}(\mathbf{x})}\right] \\
=E_{\mathbf{x} \sim p_{\text {data }}}\left[\log \frac{p_{\text {data }}(\mathbf{x})}{\frac{p_{\text {data }}(\mathbf{x})+p_{G}(\mathbf{x})}{2}}\right]+E_{\mathbf{x} \sim p_{G}}\left[\log \frac{p_{G}(\mathbf{x})}{\frac{p_{\text {data }}(\mathbf{x})+p_{G}(\mathbf{x})}{2}}\right]-\log 4 \\
=\underbrace{D_{K L}\left[p_{\text {data }}, \frac{p_{\text {data }}+p_{G}}{2}\right]+D_{K L}\left[p_{G}, \frac{p_{\text {data }}+p_{G}}{2}\right]}_{2 \times \text { Jenson-Shannon Divergence }(\text { JSD })}-\log 4 \\
=2 D_{J S D}\left[p_{\text {data }}, p_{G}\right]-\log 4
\end{gathered}
<p>
$$<br />
</p>

<p>
Btw, there are other divergences that we can use than Jenson-Shannon Divergence.<br />
</p>
</div>
</div>
<div id="outline-container-orgd01d906" class="outline-3">
<h3 id="orgd01d906">GAN training algorithm</h3>
<div class="outline-text-3" id="text-orgd01d906">
<p>
Sample minibatch of \(m\) training points \(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(m)}\) from \(\mathcal{D}\) Sample minibatch of \(m\) noise vectors \(\mathbf{z}^{(1)}, \mathbf{z}^{(2)}, \ldots, \mathbf{z}^{(m)}\) from \(p_{z}\) Update the discriminator parameters \(\phi\) by stochastic gradient ascent<br />
\[
\nabla_{\phi} V\left(G_{\theta}, D_{\phi}\right)=\frac{1}{m} \nabla_{\phi} \sum_{i=1}^{m}\left[\log D_{\phi}\left(\mathbf{x}^{(i)}\right)+\log \left(1-D_{\phi}\left(G_{\theta}\left(\mathbf{z}^{(i)}\right)\right)\right)\right]
\]<br />
Update the generator parameters \(\theta\) by stochastic gradient descent<br />
\[
\nabla_{\theta} V\left(G_{\theta}, D_{\phi}\right)=\frac{1}{m} \nabla_{\theta} \sum_{i=1}^{m} \log \left(1-D_{\phi}\left(G_{\theta}\left(\mathbf{z}^{(i)}\right)\right)\right)
\]<br />
Repeat for fixed number of epochs&#x2026;or until samples look good, lol.<br />
</p>

<p>
GANs can be <i>very</i> challenging to train in practice.<br />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<!-- copyright Ketan agrawal - on line below-->
<p>&copy; Ketan Agrawal, 2024. <a href="https://x.com/_ketan0">@_ketan0</a>.</p>
<script src="popper.min.js"></script>
<script src="tippy-bundle.umd.min.js"></script>
<script src="tooltips.js"></script>
<script src="setup-theme-switcher.js"></script>
<script src="insert-intext-citation.js"></script>
</div>
</body>
</html>
